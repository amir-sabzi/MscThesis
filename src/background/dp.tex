% Subsection intro. Elaborate on origins of DP. Why it is useful.
Data scientists always strive to understand the general properties of a population. 
To answer questions concerning the etiology of a disease, factors contributing to a societal phenomenon, or the consequences of an economic policy, researchers collect data from individual people. They then process this data to calculate population-level statistics and propose solutions based on these aggregated statistics.
Ensuring the privacy of individuals who have contributed to these studies is of utmost importance from both ethical and legal perspectives.
In fact, the intended goal of all scientific studies is to collect valuable information about the targeted population not individual people within the population. 
However, it is impossible to learn useful information about a population while learning \textit{nothing} about individuals, leading to a paradox between usefulness of a dataset (\ie utility) and its privacy. 
Differential Privacy (DP) addresses this paradox by quantifying the extent of privacy leakage for individuals in a dataset when publishing its statistics.
Within the framework of Differential Privacy (DP), the aforementioned paradox transforms into an adjustable trade-off between the utility of data and the preservation of privacy. 

In the rest of this section, we provide the formal definition of differential privacy, highlight its main properties, and explore its potential application in the domain of traffic shaping.


%
%% 
%% Definition
%%
%


\subsection{Definitions}
A database $D$ has a fixed number of entries, where each individual data point is stored in one entry.
Following the notation proposed by Dwork \etal \cite{dwork2014algorithmic}, we represent databases by their histograms: $D \in \dbSpace$. 
In this representation, every entry $D_i$ of the database $D$ represents the number elements of type $i \in \mathcal{X}$.
\\  
Within the DP context, we define a \textit{query} as a function that operates on a database. 
Any computation performed on a database, regardless of its codomain, can be regarded as a query executed on that database.
The primary objective of differential privacy is to offer meaningful population-level information for queries executed on a database, all while protecting the privacy of individual data points within the database.
Intuitively, small changes in a database should not significantly impact the outcome of a query.
To further clarify the terms "small changes" and "significant impact", we provide definitions for neighboring databases and query function sensitivity, respectively.
\\
We measure the difference between two databases with a distance metric $\rho(D, D')$.
\begin{definition}[Neighboring databases]
  Given a distance metric $\rho$, we refer to two databases $D, D' \in \dbSpace$ as neighboring databases, if and only if $\rho(D, D') \leq 1$.             
\end{definition}
\noindent In the standard DP definition, the distance metric is simply the number of different data entries in two databases (\ie Hamming distance).
However, Chatzikokolakis \etal \cite{chatzikokolakis2013broadening} show that DP applies to a general definition of distance metrics.
\\
To quantify the extent to which a single data point can influence the outcome of a query in worst-case, we introduce the concept of sensitivity. 
\begin{definition}[$l_p$-Sensitivity]
  The $l_p$-sensitivity of a query function $f$ is:
  \begin{equation*}
    \Delta f = \max_{D, D'} \|f(D) - f(D')\|_p 
  \end{equation*}
where $\rho(D, D') \leq 1$ (\ie $D$ and $D'$ are neighboring databases).
\end{definition}
\noindent
Differential Privacy involves randomization of query outputs. We define a randomized algorithm as follows.
\begin{definition}[Randomized algorithm]
  A randomized algorithm $M$ with the domain $A$ and discrete range $B$, for any given $a \in A$ and $b \in B$ outputs $M(a)= b$ with probability of $(M(a))_b$.
\end{definition}

With all the necessary components in place, we are now prepared to present the formal definition of differential privacy.
\begin{definition}[Differential privacy]
  A randomized algorithm $M: \dbSpace \rightarrow \mathbb{R}$ is $(\varepsilon, \delta)$-differentially private if for all ${S} \subseteq Range(M)$ and for  all $D, D' \in \dbSpace$ such that $\rho(D, D') \leq 1$, we have:
  \begin{equation*}
    \Pr[M(D) \in S] \leq \exp(\varepsilon)\Pr[M(D') \in S] + \delta
  \end{equation*}
\end{definition}
\noindent
Indeed, Differential Privacy (DP) is a definition rather than a specific algorithm.
It provides a framework for ensuring privacy guarantees in various randomized algorithms. Multiple randomized algorithms can achieve $(\varepsilon, \delta)$-privacy for a given set of databases, each with different characteristics. 
Intuitively, given an output, with probability $1-\delta$ the log likelihood ratio of running the algorithm $M$ on databases $D$ and $D'$ is bounded by $\varepsilon$.
This bound ensures that the presence or absence of any individual's data in the database has a limited impact on the likelihood of obtaining a particular output.
The smaller $\varepsilon$ implies that both neighboring databases are equally likely to generate the output, resulting in more privacy.
The parameter $\varepsilon$ is commonly referred to as \textit{privacy loss} within the context of DP.
The parameter $\delta$ determines the failure probability of a differential private mechanism and is typically expected to have a small value (\ie smaller than $10^{-5}$).


%
%% 
%% Properties
%%
%

\subsection{Properties}\label{subsec:dp-properties}
In this section, we explore the key properties of Differential Privacy as a privacy framework.


% ROBUSTNESS TO AUXILIARY INFORMATION.
\subsubsection{Robustness to auxiliary information}
\label{subsubsec:dp-auxiliary}
The definition of Differential Privacy does not make any assumptions regarding the prior knowledge of the adversary. 
In other words, regardless of the adversary's prior knowledge, the information gained by the adversary after observing the output of a differentially private algorithm $M$ remains within the bounds specified by Differential Privacy. 
\begin{proposition}
  \label{prop:auxiliary}
  Assume that the adversary has a prior $\Pr(D)$ over the set of all possible databases $D, D' \in \dbSpace$, given the output $S$ of a $(\varepsilon, \delta)$-DP algorithm $M: \dbSpace \rightarrow \mathbb{R}$, for all $D, D' \in \dbSpace$ such that $\rho(D, D') \leq 1$, we have: 
  \begin{equation*}
    \frac{\Pr(D|S)}{\Pr(D'|S)} \leq \exp(\varepsilon) \frac{\Pr(D)}{\Pr(D')}
  \end{equation*}
\end{proposition}
In broad terms, robustness to auxiliary information in the context of Differential Privacy is similar to the security semantics of cryptographic algorithms. 
For instance, consider a scenario where an adversary possesses the knowledge that the content of an encrypted message is either a picture of a car or a picture of a tree.
In this case, observing the encrypted message does not provide any additional evidence to indicate which of the two possibilities is more likely to be the true message.
In DP, nevertheless, observing the results of private queries does change the prior knowledge of the adversary.
However, this change remains within the boundaries defined by DP and does not exceed them.

% POST-PROCESSING
\subsubsection{Post-processing}
\label{subsubsec:post-processing}
Differential Privacy guarantees are resilient to post-processing of the output from a DP algorithm.
In fact, in the absence of any additional knowledge, the adversary is unable to undermine the guarantees of Differential Privacy simply by processing the output of the algorithm.
\begin{proposition}
  \label{prop:post-processing}
  Consider $M: \dbSpace \rightarrow R$ as a $(\varepsilon, \delta)$-differentially private algorithm, and let $g: \mathbb{R} \rightarrow \mathbb{R}$ be an arbitrary randomized mapping, then $g(M(.)): \dbSpace \rightarrow \mathbb{R}$ is $(\varepsilon, \delta)$-differentially private. 
\end{proposition}
\noindent
This implies that regardless of the complexity of a procedure, as long as the inputs are differentially private, the output is guaranteed to maintain the same level privacy.
The post-processing property of Differential Privacy can be especially advantageous when dealing with systems that have information bottlenecks, such as situations where results from multiple calculations are aggregated at a single stage.
In such systems, by applying a differentially private algorithm to the information bottleneck, the guarantee of Differential Privacy extends to the output of the entire system. 
We specifically utilize post-processing property of Differential Privacy in the design of both the {\sys} shaping mechanism and the {\sys} middlebox. {\addref}  

% PRESERVATION UNDER ADAPTIVE SEQUENTIAL COMPOSITION.
\subsubsection{Self-Composition}
The simultaneous release of results from multiple differentially private algorithms maintains the differential privacy guarantee.
This property facilitates the modular construction of differentially private algorithms, allowing multiple DP algorithms to be combined to create more sophisticated and advanced algorithms.
Furthermore, the composition theorem enables us to compute the privacy parameters associated with the sequential release of a differentially private algorithm output, thereby facilitating multiple releases of the same DP output.
There are multiple variants of composition theorem, and we with the simplest form of composition.
\begin{proposition}[Basic composition theorem]
\label{prop:basic-composition}
  Let $M_i: \dbSpace \rightarrow \mathbb{R}$ be an $(\varepsilon_i, \delta)$-differentially private algorithm for $i \in [k]$. Then, the combination of these $k$ algorithms, $M_{[k]}: \dbSpace \rightarrow \Pi_{i=1}^{k}\mathbb{R}$ is $(\sum_{i=1}^{k}\varepsilon_i, \sum_{i=1}^{k}\delta_i)$-differentially private.  
\end{proposition}
Basic composition theorem states that the privacy loss resulting from the combination of multiple differentially private algorithms is equal to the aggregate of their individual privacy losses.
While functional, the basic composition theorem tends to overestimate the privacy loss of combined DP algorithms.
The advanced composition theorem offers a more precise and rigorous bound for the aggregated privacy loss incurred by multiple differentially private algorithms. 
\begin{proposition}[Advanced composition theorem]
\label{prop:advanced-composition}
  Let $M_i: \dbSpace \rightarrow \mathbb{R}$ be an $(\varepsilon, \delta)$-differentially private algorithm for $i \in [k]$. Then, the combination of these $k$ algorithms, $M_{[k]}: \dbSpace \rightarrow \Pi_{i=1}^{k}\mathbb{R}$ is $(\varepsilon', k\delta+\delta')$-differentially such that:
  \begin{equation*}
    \forall \delta' \geq 0: \varepsilon' = \sqrt{2k\ln(1/\delta')}\varepsilon + k\varepsilon(e^{\varepsilon} - 1)
  \end{equation*}
\end{proposition}
It is important to note that this form of composition theorem is only applicable to differentially private algorithms that share the same privacy parameters.
More sophisticated versions of the composition theorem exist, offering tighter privacy bounds~\cite{kairouz2015composition, mironov2017renyi}.
We particularly use R{\'e}nyi Differential privacy to calculate the privacy loss of our differentially private shaping mechanism.